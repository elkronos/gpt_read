# retrieval_mode.R

#' Split Text into Minimal Chunks Based on Token Limit
#'
#' This function splits a document's text into as few chunks as possible without exceeding
#' the provided token limit. The splitting is performed first by paragraphs (using double newlines)
#' and then by individual words if a single paragraph exceeds the limit.
#'
#' @param text A character string containing the full document text.
#' @param token_limit An integer specifying the maximum allowed tokens per chunk.
#' @return A character vector where each element is a text chunk that does not exceed the token limit.
#' @details The function uses \code{estimate_token_count} (assumed to be available from your utils)
#' to estimate token counts.
#' @export
chunk_text_minimal <- function(text, token_limit) {
  paragraphs <- unlist(strsplit(text, "\n\n"))
  paragraphs <- paragraphs[nchar(trimws(paragraphs)) > 0]
  chunks <- c()
  current_chunk <- ""
  for (para in paragraphs) {
    candidate <- if (nzchar(current_chunk)) paste(current_chunk, para, sep = "\n\n") else para
    if (estimate_token_count(candidate) <= token_limit) {
      current_chunk <- candidate
    } else {
      if (nzchar(current_chunk)) {
        chunks <- c(chunks, current_chunk)
      }
      # If a single paragraph exceeds the token limit, split it by words
      if (estimate_token_count(para) > token_limit) {
        words <- unlist(strsplit(para, "\\s+"))
        temp_chunk <- ""
        for (word in words) {
          candidate2 <- if (nzchar(temp_chunk)) paste(temp_chunk, word) else word
          if (estimate_token_count(candidate2) <= token_limit) {
            temp_chunk <- candidate2
          } else {
            chunks <- c(chunks, temp_chunk)
            temp_chunk <- word
          }
        }
        current_chunk <- temp_chunk
      } else {
        current_chunk <- para
      }
    }
  }
  if (nzchar(current_chunk)) {
    chunks <- c(chunks, current_chunk)
  }
  return(chunks)
}

#' Retrieve Answer from Document Using GPT with Retrieval Mode
#'
#' This function extracts relevant text from the document to answer a given question.
#' It first checks whether the document's full text exceeds the allowed token limit for a single API call.
#' If it does, the function uses \code{chunk_text_minimal} to split the text into minimal chunks.
#' For each chunk, a targeted "skimming" prompt extracts only the information relevant to the question.
#' The skimmed results are then combined and used to generate the final answer. If the full text
#' fits within the token limit, a direct retrieval prompt is used.
#'
#' @param chunks A character vector of text chunks (e.g. generated by a parsing function).
#' @param question A character string representing the question to answer.
#' @param model A character string specifying the GPT model to use (default: "gpt-3.5-turbo").
#' @param temperature A numeric value for the sampling temperature (default: 0.0).
#' @param max_tokens Maximum tokens for each API response; if NULL, defaults to the model's output limit.
#' @param presence_penalty A numeric value for the presence penalty (default: 0.0).
#' @param frequency_penalty A numeric value for the frequency penalty (default: 0.0).
#' @param num_retries An integer specifying the number of retries for API calls (default: 5).
#' @param pause_base A numeric value (in seconds) specifying the base pause time for retry backoff (default: 3).
#' @param use_parallel Logical; if TRUE, processes chunks in parallel (default: FALSE).
#' @param fallback Logical; if TRUE, falls back to generic chunked mode if no relevant text is extracted (default: TRUE).
#' @param return_json Logical; if TRUE, returns a JSON string containing the full chain-of-thought; otherwise, only the final answer is returned.
#' @param ... Additional arguments that will be passed to underlying functions.
#' @return A character string containing the final answer, or a JSON string with detailed chain-of-thought information if \code{return_json} is TRUE.
#' @import jsonlite
#' @export
gpt_read_retrieval <- function(chunks, question, model = "gpt-3.5-turbo", temperature = 0.0, 
                               max_tokens = NULL, presence_penalty = 0.0, frequency_penalty = 0.0,
                               num_retries = 5, pause_base = 3, use_parallel = FALSE, fallback = TRUE, 
                               return_json = FALSE, ...) {
  chain <- list()  # container for chain-of-thought details
  
  # Combine all chunks into one full text
  full_text <- paste(chunks, collapse = "\n\n")
  chain$combined_text <- full_text
  
  model_limits <- get_model_limits(model)
  if (is.null(max_tokens)) max_tokens <- model_limits$output_tokens
  # Calculate allowed input tokens for a single API call.
  # Decrease the window size by subtracting 2000 tokens to account for skimming responses and question tokens.
  allowed_input_tokens <- model_limits$context_window - max_tokens - 2000  
  
  if (estimate_token_count(full_text) > allowed_input_tokens) {
    chain$mode <- "chunked_retrieval"
    chain$initial_message <- "Document exceeds allowed token limit for a single retrieval query. Splitting into minimal chunks..."
    
    # Set a safe token limit for each minimal chunk (e.g., maximum 2000 tokens)
    safe_token_limit <- min(allowed_input_tokens, 2000)
    retrieval_chunks <- chunk_text_minimal(full_text, safe_token_limit)
    chain$retrieval_chunks <- retrieval_chunks
    
    # For each minimal chunk, perform targeted skimming with dynamic max_tokens adjustment
    skim_results <- list()
    for (i in seq_along(retrieval_chunks)) {
      chunk <- retrieval_chunks[i]
      skim_prompt <- paste(
        "Skim the following document excerpt and extract only the information relevant to the question.",
        "Omit any parts that are not helpful for answering.",
        "\n\nQuestion:\n", question,
        "\n\nExcerpt:\n", chunk
      )
      # Compute how many tokens are already in the prompt
      prompt_tokens <- estimate_token_count(skim_prompt)
      available_tokens <- max(model_limits$context_window - prompt_tokens - 50, 50)
      skim_max_tokens <- min(max_tokens, available_tokens)
      
      messages <- list(
        list(role = "system", content = "You are an assistant that skims document text for relevance."),
        list(role = "user", content = skim_prompt)
      )
      step_result <- process_api_call(messages, model = model, temperature = temperature, max_tokens = skim_max_tokens,
                                      presence_penalty = presence_penalty, frequency_penalty = frequency_penalty,
                                      num_retries = num_retries, pause_base = pause_base)
      skim_results[[paste0("chunk_", i)]] <- list(
        prompt = skim_prompt,
        response = step_result
      )
    }
    chain$skim_results <- skim_results
    
    # Combine skimmed results from each chunk
    combined_relevant_text <- paste(sapply(skim_results, function(x) x$response), collapse = "\n\n")
    chain$combined_relevant_text <- combined_relevant_text
    
    # Final step: answer the question based on the combined skimmed text
    final_prompt <- paste(
      "Based on the following extracted relevant information from a document, answer the question:",
      "\n\nRelevant Information:\n", combined_relevant_text,
      "\n\nQuestion:\n", question
    )
    final_prompt_tokens <- estimate_token_count(final_prompt)
    available_tokens_final <- max(model_limits$context_window - final_prompt_tokens - 50, 50)
    final_max_tokens <- min(max_tokens, available_tokens_final)
    
    messages_final <- list(
      list(role = "system", content = "You are a research assistant who answers questions based solely on provided relevant information."),
      list(role = "user", content = final_prompt)
    )
    final_answer <- process_api_call(messages_final, model = model, temperature = temperature, max_tokens = final_max_tokens,
                                     presence_penalty = presence_penalty, frequency_penalty = frequency_penalty,
                                     num_retries = num_retries, pause_base = pause_base)
    chain$final_step <- list(
      prompt = final_prompt,
      response = final_answer
    )
    chain$final_answer <- final_answer
  } else {
    chain$mode <- "single_retrieval"
    # Use original retrieval approach if the text fits in one query.
    relevant_prompt <- paste(
      "You are an assistant that helps extract only the sections of a document that are relevant",
      "to answering a given question. Return the text snippets that are most pertinent to the question,",
      "and omit any parts that are not helpful for answering.",
      "\n\nQuestion:\n", question,
      "\n\nDocument:\n", full_text
    )
    relevant_prompt_tokens <- estimate_token_count(relevant_prompt)
    available_tokens_single <- max(model_limits$context_window - relevant_prompt_tokens - 50, 50)
    single_max_tokens <- min(ifelse(is.null(max_tokens), 2048, max_tokens), available_tokens_single)
    
    messages_filter <- list(
      list(role = "system", content = "You selectively extract relevant text for a question."),
      list(role = "user", content = relevant_prompt)
    )
    relevant_text <- process_api_call(messages_filter, model = model, temperature = 0.0, 
                                      max_tokens = single_max_tokens,
                                      presence_penalty = presence_penalty, frequency_penalty = frequency_penalty,
                                      num_retries = num_retries, pause_base = pause_base)
    chain$extraction_step <- list(
      prompt = relevant_prompt,
      response = relevant_text
    )
    
    if (is.null(relevant_text) || nchar(trimws(relevant_text)) == 0) {
      warning("No relevant text extracted by GPT. Falling back to generic chunked mode.")
      fallback_answer <- gpt_read_chunked(chunks, question, model = model, temperature = temperature, max_tokens = max_tokens,
                                          presence_penalty = presence_penalty, frequency_penalty = frequency_penalty,
                                          num_retries = num_retries, pause_base = pause_base, use_parallel = use_parallel)
      chain$fallback <- fallback_answer
      chain$final_answer <- fallback_answer
    } else {
      messages_answer <- list(
        list(role = "system", content = "You are a research assistant who answers questions based solely on the provided excerpts."),
        list(role = "user", content = paste("Relevant Text:\n", relevant_text)),
        list(role = "user", content = paste("Question:\n", question))
      )
      answer <- process_api_call(messages_answer, model = model, temperature = temperature, max_tokens = max_tokens,
                                 presence_penalty = presence_penalty, frequency_penalty = frequency_penalty,
                                 num_retries = num_retries, pause_base = pause_base)
      chain$answer_step <- list(
        prompt = paste("Relevant Text:\n", relevant_text, "\nQuestion:\n", question),
        response = answer
      )
      chain$final_answer <- answer
    }
  }
  
  if (return_json) {
    return(jsonlite::toJSON(chain, pretty = TRUE, auto_unbox = TRUE))
  } else {
    return(chain$final_answer)
  }
}
